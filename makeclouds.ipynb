{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(csv_name, mask_png, num_rows, baseline = None):\n",
    "\n",
    "  #Generates and plots a wordcloud for a dataset. Optionally rescales word weights to show their relative frequency to a baseline dataset.\n",
    "\n",
    "  #Parameters:\n",
    "    #csv_name: a csv file containing the tweets to be analyzed\n",
    "    #mask_png: a png of the logo to use as the coloring scheme\n",
    "    #num_rows: how many rows of the data to use(use len(csv_name)) to use all rows\n",
    "    #baseline(optional): a csv file of data (competitors or overall category) to compare starbucks word frequencies to\n",
    "\n",
    "    # imports\n",
    "    from nltk.corpus import words\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from spacy.lang.en import English\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from nltk import download\n",
    "    download('wordnet')\n",
    "    from nltk.corpus import stopwords\n",
    "    from wordcloud import WordCloud, ImageColorGenerator\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import base64\n",
    "\n",
    "    # read and inspect data (optional)\n",
    "    df1 = csv_name\n",
    "    if baseline is not None:\n",
    "      df2 = pd.read_csv(baseline)\n",
    "      df2 = df2.head(num_rows)\n",
    "    #if you want to test with a small subset to save time\n",
    "    #df1 = df1.head(num_rows)\n",
    "    \n",
    "\n",
    "    def clean_and_return_TFIDF_weights(df):\n",
    "      #replace non-alphabetical characters with space using Regex\n",
    "      df['tweet'] = df['tweet'].map(lambda x: re.sub(r'[^a-zA-Z] ', ' ', str(x)))\n",
    "      #eliminate rows with empty values for tweet by dropping na's\n",
    "      df = df.dropna(subset=['tweet'])\n",
    "      #stopwords - NLTK\n",
    "      download('stopwords')\n",
    "      #additional stopwords based on previous wordcloud results\n",
    "      more_stopwords = [\"starbucks\",\"want\", \"coffee\", \"like\", \"say\", \"put\" , \"nestl\", \"nestle\", \"nestlÃ©\", \"starbuck\", \"starbucks\", \"https\", \"cc\", \"co\", \"ht\", \"tps\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\"Mr\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "      stop = stopwords.words('english') + more_stopwords\n",
    "\n",
    "      # remove encoding of word- strip off any unwanted formatting/http://\n",
    "      def remove_encoding_word(word):\n",
    "          word = str(word)\n",
    "          word = word.encode('ASCII', 'ignore').decode('ASCII')\n",
    "          return word\n",
    "      # apply remove_encoding_word to each word in text\n",
    "      def remove_encoding_text(text):\n",
    "          text = str(text)\n",
    "          text = ' '.join(remove_encoding_word(word) for word in text.split() if word not in stop)\n",
    "          return text\n",
    "      # apply remove_encoding_word and create lemmatizer \n",
    "      df['tweet'] = df['tweet'].apply(remove_encoding_text)\n",
    "      text = ' '.join(words for words in df['tweet'])\n",
    "      lemma = WordNetLemmatizer().lemmatize\n",
    "      # apply lemmatizer (as opposed to stemming, lemmatizing breaks words down into similar dictionary definitions), filter short words and nonalphabetical characters, and fit TF-IDF model\n",
    "      def tokenize(document):\n",
    "        tokens = [lemma(w) for w in document.split() if len(w) > 3 and w.isalpha()]\n",
    "        return tokens\n",
    "\n",
    "      vectorizer = TfidfVectorizer(tokenizer = tokenize, ngram_range = ((1,1)),\n",
    "                                  stop_words = stop, strip_accents = 'unicode')\n",
    "\n",
    "      # fit vectorizer and transform tweets column (safe to ignore warning!)\n",
    "      tdm = vectorizer.fit_transform(df['tweet'])\n",
    "      # view words\n",
    "      vectorizer.vocabulary_.items()\n",
    "      #calculate TF-IDF weights - fast\n",
    "      n = 1000\n",
    "      items = list(vectorizer.vocabulary_.items())\n",
    "      y = [dict(items[x:x+n+1]) for x in range(0, len(vectorizer.vocabulary_), n+1)]\n",
    "      tfidf_weights = []\n",
    "      counter = 0\n",
    "      for d in y:\n",
    "        counter += 1\n",
    "        tfidf_weights.extend([(word, tdm.getcol(idx).sum()) for word, idx in d.items()])\n",
    "        print(\"Processing subdictionary:\", counter, \"of\", len(y))\n",
    "      return tfidf_weights\n",
    "\n",
    "    #get TF-IDF weights\n",
    "    tfidf_weights_primary = clean_and_return_TFIDF_weights(df1)\n",
    "    if baseline is not None:\n",
    "      tfidf_weights_baseline = clean_and_return_TFIDF_weights(df2)\n",
    "\n",
    "    #calculate TF-IDF weights - slow\n",
    "    '''tfidf_weights = [(word, tdm.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    len(tfidf_weights)\n",
    "    tfidf_weights[0:10]'''\n",
    "\n",
    "    #rescale TF-IDF weights\n",
    "    if baseline is None:\n",
    "          tfidf_weights_rescaled = tfidf_weights_primary\n",
    "    else:\n",
    "      tfidf_weights_rescaled = []\n",
    "      tfidf_weights_primary_dict = dict(tfidf_weights_primary)\n",
    "      tfidf_weights_baseline_dict = dict(tfidf_weights_baseline)\n",
    "      for weight, word in enumerate(tfidf_weights_primary_dict):\n",
    "        if word in tfidf_weights_baseline_dict.keys():\n",
    "          rescaled_weight = (weight + 0.00000000000000000000000000001)/(tfidf_weights_baseline_dict[word] + 0.00000000000000000000000000001)\n",
    "          tfidf_weights_rescaled.append((word,rescaled_weight))\n",
    "        else:\n",
    "          tfidf_weights_rescaled.append((word,weight))\n",
    "\n",
    "\n",
    "    #Create Word Cloud\n",
    "    #a) including link to .png file in create_wordcloud command will turn the provided image into a mask for the wordcloud\n",
    "    twitter_mask2 = np.array(Image.open(mask_png))\n",
    "    image_colors = ImageColorGenerator(twitter_mask2)\n",
    "    w = WordCloud(width=1500, height=1200, mask = twitter_mask2, background_color='white',\n",
    "                max_words=2000).fit_words(dict(tfidf_weights_rescaled))\n",
    "    plot = plt.figure(figsize=(20,15))\n",
    "    plt.imshow(w.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig('tweets_wordcloud.png')\n",
    "    \n",
    "    # encode the image into source codes that dash can read\n",
    "    image_filename = 'tweets_wordcloud.png'\n",
    "    encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "    source='data:image/png;base64,{}'.format(encoded_image.decode())\n",
    "    \n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
